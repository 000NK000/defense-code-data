{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM, Trainer, TrainingArguments\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased').to(device)\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=512)\n",
        "\n",
        "\n",
        "def load_and_preprocess_data():\n",
        "    with open('/content/cleaned_train.txt', 'r', encoding='utf-8') as file:\n",
        "        texts = file.readlines()\n",
        "    return Dataset.from_list([{'text': text.strip()} for text in texts])\n",
        "\n",
        "unlabeled_dataset = load_and_preprocess_data()\n",
        "unlabeled_dataset = unlabeled_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/results',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=32,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    learning_rate=2e-6,\n",
        "    logging_dir='/content/logs',\n",
        "    logging_steps=500,\n",
        "    report_to=\"none\",\n",
        "    fp16=True\n",
        "\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = predictions.argmax(axis=-1)\n",
        "\n",
        "\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "\n",
        "\n",
        "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
        "\n",
        "\n",
        "    precision = precision_score(labels, predictions, average='macro', zero_division=0)\n",
        "    recall = recall_score(labels, predictions, average='macro', zero_division=0)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'macro_f1': macro_f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=unlabeled_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "\n",
        "classification_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3).to(device)\n",
        "classification_model.load_state_dict(model.state_dict(), strict=False)\n",
        "\n",
        "\n",
        "def load_test_data():\n",
        "    texts = []\n",
        "    labels = []\n",
        "    with open('/content/processed_test.txt', 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            *text, label = line.strip().split()\n",
        "            labels.append(int(label))\n",
        "            texts.append(' '.join(text))\n",
        "    return Dataset.from_dict({'text': texts, 'label': labels})\n",
        "\n",
        "test_dataset = load_test_data()\n",
        "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "\n",
        "test_trainer = Trainer(\n",
        "    model=classification_model,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "\n",
        "eval_results = test_trainer.evaluate(eval_dataset=test_dataset)\n",
        "print(eval_results)\n",
        "\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
        "    outputs = classification_model(**inputs)\n",
        "    prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
        "    return \"Positive\" if prediction == 2 else \"Neutral\" if prediction == 1 else \"Negative\"\n",
        "\n",
        "\n",
        "sample_text = \"This movie was fantastic! I loved the acting and the story.\"\n",
        "print(\"Prediction:\", predict_sentiment(sample_text))\n"
      ],
      "metadata": {
        "id": "wZWVnWCiaUpp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}